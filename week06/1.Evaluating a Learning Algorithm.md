# Evaluating a Learning Algorithm

## Deciding what to try next   

However,    when    you    test    your    hypothesis    on    a    new    set    of    houses,    you    find    that    
it    makes    unacceptably    large    errors    in    its    predicDons.    What    should    you    try    next? 

Get    more    training    examples    
Try    smaller    sets    of    features    
Try    geJng    addiDonal    features    
Try    adding    polynomial    features    
Try    decreasing  λ  
Try    increasing  λ

Machine    learning    diagnos5c:    
* DiagnosDc:    A    test    that    you    can    run    to    gain    insight    what    
is/isn’t    working    with    a    learning    algorithm,    and    gain    
guidance    as    to    how    best    to    improve    its    performance.    
* DiagnosDcs    can    take    Dme    to    implement,    but    doing    so    
can    be    a    very    good    use    of    your    Dme.  

## Evaluating a Hypothesis

Once we have done some trouble shooting for errors in our predictions by:

    Getting more training examples
    Trying smaller sets of features
    Trying additional features
    Trying polynomial features
    Increasing or decreasing λ

We can move on to evaluate our new hypothesis.

A hypothesis may have a low error for the training examples but still be inaccurate (because of overfitting). Thus, to evaluate a hypothesis, given a dataset of training examples, we can split up the data into two sets: a training set and a test set. Typically, the training set consists of 70 % of your data and the test set is the remaining 30 %.

The new procedure using these two sets is then:

    Learn Θ and minimize Jtrain(Θ) using the training set
    Compute the test set error Jtest(Θ)

The test set error

    For linear regression: Jtest(Θ)=12mtest∑i=1mtest(hΘ(xtest(i))−ytest(i))2
    For classification ~ Misclassification error (aka 0/1 misclassification error):

err(hΘ(x),y)=1if hΘ(x)≥0.5 and y=0 or hΘ(x)<0.5 and y=10otherwise

This gives us a binary 0 or 1 error result based on a misclassification. The average test error for the test set is:

Test Error=1mtest∑i=1mtesterr(hΘ(xtest(i)),ytest(i))

This gives us the proportion of the test data that was misclassified.

## Model selecDon and training/validaDon/test sets   

Model selection 
Evaluating your hypothesis 


